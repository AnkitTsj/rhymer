{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6391fa79",
   "metadata": {},
   "source": [
    "### Necessary Imports and set-up of the paths like - \n",
    "base model path, adapter path, dataset path, model save path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import gc \n",
    "BASE_MODEL_NAME = \"unsloth/zephyr-sft-bnb-4bit\"\n",
    "FINAL_ADAPTER_PATH = \"E:/papers-i-implement/LoRA/main_scripts/results_optimized_all_tones_dynamic/final_model\"\n",
    "SMALL_ADAPTER_PATH = \"E:/papers-i-implement/LoRA/models/results_optimized_all_tones/final_model\"\n",
    "FINETUNED_ADAPTER_PATH = \"E:/papers-i-implement/LoRA/main_scripts/results_optimized_all_tones_dynamic/checkpoint-1200\" \n",
    "DATA_TEST_PATH = \"E:/papers-i-implement/poet/data/conversation_data_test.json\"\n",
    "MAX_SEQ_LENGTH = 2048 \n",
    "\n",
    "NUM_COMPARISON_SAMPLES = 10\n",
    "COMPARISON_OUTPUT_FILE = \"post_hoc_model_comparison_sequential_2.json\"\n",
    "PERPLEXITY_BATCH_SIZE = 2 \n",
    "TARGET_COMPARISON_TONE = \"poetic\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321aa91c",
   "metadata": {},
   "source": [
    "### Model & Tokenizer Loader\n",
    "This function loads either:\n",
    "\n",
    "A base model (optionally in 4-bit for efficiency), or A fine-tuned adapter (LoRA) on top of a base model.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Uses 4-bit quantization (nf4, bfloat16) if enabled.\n",
    "\n",
    "Handles missing eos_token or pad_token in tokenizer.\n",
    "\n",
    "Automatically maps to available device (GPU/CPU).\n",
    "\n",
    "Cleans up memory on failure when loading adapters.\n",
    "\n",
    "Returns:\n",
    "(model, tokenizer) â€” ready for evaluation or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_tokenizer_for_analysis(model_name_or_path, is_adapter_path=False, base_model_name_for_adapter=None, load_in_4bit=True):\n",
    "    print(\"-\" * 50)\n",
    "    if is_adapter_path:\n",
    "        print(f\"Loading FINE-TUNED model (Base: {base_model_name_for_adapter} + Adapters: {model_name_or_path})\")\n",
    "        if base_model_name_for_adapter is None:\n",
    "             raise ValueError(\"base_model_name_for_adapter must be provided when loading adapters.\")\n",
    "    \n",
    "        base_model, tokenizer = load_model_tokenizer_for_analysis(base_model_name_for_adapter, load_in_4bit=load_in_4bit)\n",
    "        \n",
    "        try:\n",
    "             model = PeftModel.from_pretrained(base_model, model_name_or_path)\n",
    "             print(\"LoRA adapters loaded successfully.\")\n",
    "             return model, tokenizer\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading adapters from {model_name_or_path}: {e}\")\n",
    "            del base_model, tokenizer\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            raise e \n",
    "    else:\n",
    "        print(f\"Loading BASE model: {model_name_or_path}\")\n",
    "        bnb_config = None\n",
    "        dtype = None\n",
    "        if load_in_4bit:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            dtype = torch.bfloat16\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            quantization_config=bnb_config,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=\"auto\", \n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "        if tokenizer.eos_token is None: tokenizer.add_special_tokens({'eos_token': '<|endoftext|>'})\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            if hasattr(model, 'config'): \n",
    "                 model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        print(f\"Model {model_name_or_path} and tokenizer loaded onto device: {model.device}\")\n",
    "        return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933d863",
   "metadata": {},
   "source": [
    "### Text Generation Function\n",
    "Generates responses from the model based on user prompts and a target tone.\n",
    "\n",
    "Key Steps:\n",
    "\n",
    "Formats each prompt with USER, TONE, and ASSISTANT tags.\n",
    "\n",
    "Uses model.generate() with top-k/top-p sampling for creative outputs.\n",
    "\n",
    "Truncates long inputs to half the max sequence length.\n",
    "\n",
    "Returns a list of generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(model, tokenizer, prompts, tone, max_new=256):\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prompt in tqdm(prompts, desc=f\"Generating ({tone})\"):\n",
    "            formatted_prompt = f\"USER: {prompt}\\n TONE:{tone} \\nASSISTANT: \"\n",
    "            inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH // 2).to(model.device)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                do_sample=True, top_k=50, top_p=0.9\n",
    "            )\n",
    "            output_text = tokenizer.decode(generated_ids[0, inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "            outputs.append(output_text.strip())\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1577e33",
   "metadata": {},
   "source": [
    "### Perplexity Calculation\n",
    "Evaluates how well the model predicts a given dataset by computing perplexity.\n",
    "\n",
    "How it works:\n",
    "\n",
    "Batches the tokenized inputs and pads them.\n",
    "\n",
    "Computes loss using the model in evaluation mode.\n",
    "\n",
    "Aggregates total loss and valid tokens across batches.\n",
    "\n",
    "Returns perplexity = exp(avg_loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_perplexity(model, tokenizer, tokenized_dataset, batch_size=4):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    print(f\"\\nCalculating perplexity with batch size {batch_size}...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(tokenized_dataset), batch_size), desc=\"Perplexity Batches\"):\n",
    "            batch_indices = range(i, min(i + batch_size, len(tokenized_dataset)))\n",
    "            batch_texts = [tokenized_dataset[j]['input_ids'] for j in batch_indices]\n",
    "\n",
    "            batch_padded = tokenizer.pad({\"input_ids\": batch_texts}, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            outputs = model(**batch_padded, labels=batch_padded[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "\n",
    "            valid_tokens_in_batch = batch_padded[\"attention_mask\"].sum().item()\n",
    "            if valid_tokens_in_batch > 0:\n",
    "                total_loss += loss.item() * valid_tokens_in_batch\n",
    "                total_tokens += valid_tokens_in_batch\n",
    "            del batch_padded, outputs, loss\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        print(\"Warning: No valid tokens found for perplexity calculation.\")\n",
    "        return None\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    print(f\"Calculated Average Loss: {avg_loss:.4f}\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc714b",
   "metadata": {},
   "source": [
    "### Load Comparison Prompts\n",
    "Loads a small set of prompts from the test dataset for output comparison.\n",
    "\n",
    "Key Steps:\n",
    "\n",
    "Selects NUM_COMPARISON_SAMPLES prompts from the dataset.\n",
    "\n",
    "Filters out prompts that contain \"conversation\" with an \"input\" field.\n",
    "\n",
    "Optionally retrieves reference responses in the target tone for evaluation.\n",
    "\n",
    "Outputs:\n",
    "\n",
    "comparison_prompts: List of selected prompt texts.\n",
    "\n",
    "comparison_prompts_data_refs: Corresponding target-tone references (if available).\n",
    "\n",
    "Useful for evaluating different models or adapter outputs side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9016fc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting 10 comparison samples...\n",
      "Selected 10 prompts.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1. Load Comparison Prompts\n",
    "print(f\"\\nSelecting {NUM_COMPARISON_SAMPLES} comparison samples...\")\n",
    "comparison_prompts = []\n",
    "comparison_prompts_data_refs = []\n",
    "try:\n",
    "    # Load only the necessary part of the test data\n",
    "    comparison_raw_data = load_dataset(\"json\", data_files=DATA_TEST_PATH, split=f\"train[:{NUM_COMPARISON_SAMPLES * 2}]\")\n",
    "    for example in comparison_raw_data:\n",
    "         if 'conversation' in example and 'input' in example['conversation']:\n",
    "             comparison_prompts.append(example['conversation']['input'])\n",
    "             comparison_prompts_data_refs.append(example['conversation'].get('responses', {}).get(TARGET_COMPARISON_TONE, \"N/A\"))\n",
    "         if len(comparison_prompts) >= NUM_COMPARISON_SAMPLES:\n",
    "            break\n",
    "    comparison_prompts = comparison_prompts[:NUM_COMPARISON_SAMPLES] # Ensure exact number\n",
    "    comparison_prompts_data_refs = comparison_prompts_data_refs[:NUM_COMPARISON_SAMPLES]\n",
    "    print(f\"Selected {len(comparison_prompts)} prompts.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test data for comparison: {e}\")\n",
    "    comparison_prompts = []\n",
    "    comparison_prompts_data_refs = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04120c",
   "metadata": {},
   "source": [
    "###  Load & Preprocess Test Data for Perplexity\n",
    "Prepares test data samples for evaluating model perplexity.\n",
    "\n",
    "Steps Involved:\n",
    "\n",
    "Loads the full test set from JSON.\n",
    "\n",
    "Filters and formats each example into:\n",
    "\n",
    "php-template\n",
    "Copy\n",
    "Edit\n",
    "USER: \"< input >\"\n",
    "TONE: \"< target_tone >\"\n",
    "ASSISTANT: \"< response >< eos >\"\n",
    "Only keeps samples with valid user input and target-tone response.\n",
    "\n",
    "Uses a temporary tokenizer to ensure correct formatting and special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0650e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and preprocessing test data for perplexity calculation...\n",
      "Loaded 2600 samples for perplexity calculation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Load and Preprocess Data for Perplexity\n",
    "print(\"\\nLoading and preprocessing test data for perplexity calculation...\")\n",
    "tokenized_perplexity_dataset = None\n",
    "try:\n",
    "    raw_test_data = load_dataset(\"json\", data_files=DATA_TEST_PATH, split=\"train\")\n",
    "\n",
    "    temp_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)\n",
    "    if temp_tokenizer.eos_token is None: temp_tokenizer.add_special_tokens({'eos_token': '<|endoftext|>'})\n",
    "\n",
    "    def format_prompt_ppl(example, response_type=TARGET_COMPARISON_TONE):\n",
    "        conv = example.get('conversation')\n",
    "        if not conv or 'input' not in conv or 'responses' not in conv or response_type not in conv['responses']:\n",
    "            return {\"text\": None}\n",
    "        user_input = conv['input']\n",
    "        response = conv['responses'][response_type]\n",
    "        formatted_text = f\"USER: {user_input}\\n TONE:{response_type} \\nASSISTANT: {response}{temp_tokenizer.eos_token}\"\n",
    "        return {\"text\": formatted_text}\n",
    "\n",
    "    formatted_test_data = raw_test_data.map(\n",
    "        lambda x: format_prompt_ppl(x), num_proc=1\n",
    "    ).filter(lambda x: x['text'] is not None)\n",
    "\n",
    "    perplexity_ready_data = formatted_test_data\n",
    "    print(f\"Loaded {len(perplexity_ready_data)} samples for perplexity calculation.\")\n",
    "    del temp_tokenizer \n",
    "    gc.collect()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing test data for perplexity: {e}\")\n",
    "    perplexity_ready_data = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00519c68",
   "metadata": {},
   "source": [
    "### Phase 1:\n",
    "This analysis the base model and calculates - \n",
    "perplexity, \n",
    "loss, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3235cdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Phase 1: Base Model Analysis ====================\n",
      "--------------------------------------------------\n",
      "Loading BASE model: unsloth/zephyr-sft-bnb-4bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ankit Kumar\\anaconda3\\Lib\\site-packages\\transformers\\quantizers\\auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model unsloth/zephyr-sft-bnb-4bit and tokenizer loaded onto device: cuda:0\n",
      "\n",
      "--- Generating Baseline Outputs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (poetic): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [08:51<00:00, 53.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tokenizing Perplexity Data (Base Model) ---\n",
      "--- Calculating Base Model Perplexity ---\n",
      "\n",
      "Calculating perplexity with batch size 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perplexity Batches:   0%|          | 0/1300 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Perplexity Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1300/1300 [18:30<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Average Loss: 2.3826\n",
      "Base Model Perplexity: 10.8328\n",
      "\n",
      "--- Unloading Base Model ---\n",
      "Base Model unloaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Main Analysis Steps (Sequential) ---\n",
    "\n",
    "baseline_outputs = []\n",
    "base_perplexity = None\n",
    "finetuned_outputs = []\n",
    "ft_perplexity = None\n",
    "\n",
    "# == Phase 1: Base Model Analysis ==\n",
    "print(\"\\n\" + \"=\"*20 + \" Phase 1: Base Model Analysis \" + \"=\"*20)\n",
    "if comparison_prompts or perplexity_ready_data:\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    try:\n",
    "        # Load Base Model + Tokenizer\n",
    "        model, tokenizer = load_model_tokenizer_for_analysis(BASE_MODEL_NAME)\n",
    "\n",
    "        # Generate Baseline Outputs\n",
    "        if comparison_prompts:\n",
    "            print(\"\\n--- Generating Baseline Outputs ---\")\n",
    "            baseline_outputs = generate_text(model, tokenizer, comparison_prompts, TARGET_COMPARISON_TONE)\n",
    "\n",
    "        # Calculate Base Perplexity\n",
    "        if perplexity_ready_data:\n",
    "            print(\"\\n--- Tokenizing Perplexity Data (Base Model) ---\")\n",
    "            # NOW tokenize using the loaded tokenizer\n",
    "            tokenized_perplexity_dataset = perplexity_ready_data.map(\n",
    "                 lambda examples: tokenizer(examples[\"text\"], truncation=True, max_length=MAX_SEQ_LENGTH, padding=False),\n",
    "                 batched=True, num_proc=1, remove_columns=perplexity_ready_data.column_names\n",
    "            )\n",
    "            print(\"--- Calculating Base Model Perplexity ---\")\n",
    "            base_perplexity = calculate_perplexity(model, tokenizer, tokenized_perplexity_dataset, PERPLEXITY_BATCH_SIZE)\n",
    "            print(f\"Base Model Perplexity: {base_perplexity:.4f}\" if base_perplexity else \"Base Model Perplexity: Error\")\n",
    "            del tokenized_perplexity_dataset # Clean up tokenized data for this phase\n",
    "            gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Base Model analysis: {e}\")\n",
    "    finally:\n",
    "        # *** CRITICAL: Unload Base Model ***\n",
    "        print(\"\\n--- Unloading Base Model ---\")\n",
    "        del model\n",
    "        del tokenizer\n",
    "        gc.collect() # Force garbage collection\n",
    "        torch.cuda.empty_cache() # Release GPU memory\n",
    "        print(\"Base Model unloaded.\")\n",
    "else:\n",
    "    print(\"Skipping Base Model analysis due to lack of comparison prompts or perplexity data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a4540",
   "metadata": {},
   "source": [
    "### Phase 2:\n",
    "This analysis the finetuned model and calculates - \n",
    "perplexity, \n",
    "loss, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802bffb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Phase 2: Fine-tuned Model Analysis ====================\n",
      "--------------------------------------------------\n",
      "Loading FINE-TUNED model (Base: unsloth/zephyr-sft-bnb-4bit + Adapters: E:/papers-i-implement/LoRA/main_scripts/results_optimized_all_tones_dynamic/final_model)\n",
      "--------------------------------------------------\n",
      "Loading BASE model: unsloth/zephyr-sft-bnb-4bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ankit Kumar\\anaconda3\\Lib\\site-packages\\transformers\\quantizers\\auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model unsloth/zephyr-sft-bnb-4bit and tokenizer loaded onto device: cuda:0\n",
      "LoRA adapters loaded successfully.\n",
      "\n",
      "--- Generating Fine-tuned Outputs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (poetic): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:19<00:00, 19.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tokenizing Perplexity Data (Fine-tuned Model) ---\n",
      "--- Calculating Fine-tuned Model Perplexity ---\n",
      "\n",
      "Calculating perplexity with batch size 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perplexity Batches:   0%|          | 0/1300 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Perplexity Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1300/1300 [22:09<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Average Loss: 1.7373\n",
      "Fine-tuned Model Perplexity: 5.6818\n",
      "\n",
      "--- Unloading Fine-tuned Model ---\n",
      "Fine-tuned Model unloaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# == Phase 2: Fine-tuned Model Analysis ==\n",
    "print(\"\\n\" + \"=\"*20 + \" Phase 2: Fine-tuned Model Analysis \" + \"=\"*20)\n",
    "if comparison_prompts or perplexity_ready_data:\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    try:\n",
    "        # Load Fine-tuned Model (Base + Adapters) + Tokenizer\n",
    "        model, tokenizer = load_model_tokenizer_for_analysis(\n",
    "            FINAL_ADAPTER_PATH,\n",
    "            is_adapter_path=True,\n",
    "            base_model_name_for_adapter=BASE_MODEL_NAME\n",
    "        )\n",
    "\n",
    "        # Generate Fine-tuned Outputs\n",
    "        if comparison_prompts:\n",
    "            print(\"\\n--- Generating Fine-tuned Outputs ---\")\n",
    "            finetuned_outputs = generate_text(model, tokenizer, comparison_prompts, TARGET_COMPARISON_TONE)\n",
    "\n",
    "        # Calculate Fine-tuned Perplexity\n",
    "        if perplexity_ready_data:\n",
    "            print(\"\\n--- Tokenizing Perplexity Data (Fine-tuned Model) ---\")\n",
    "            tokenized_perplexity_dataset = perplexity_ready_data.map(\n",
    "                 lambda examples: tokenizer(examples[\"text\"], truncation=True, max_length=MAX_SEQ_LENGTH, padding=False),\n",
    "                 batched=True, num_proc=1, remove_columns=perplexity_ready_data.column_names\n",
    "            )\n",
    "            print(\"--- Calculating Fine-tuned Model Perplexity ---\")\n",
    "            ft_perplexity = calculate_perplexity(model, tokenizer, tokenized_perplexity_dataset, PERPLEXITY_BATCH_SIZE)\n",
    "            print(f\"Fine-tuned Model Perplexity: {ft_perplexity:.4f}\" if ft_perplexity else \"Fine-tuned Model Perplexity: Error\")\n",
    "            del tokenized_perplexity_dataset # Clean up tokenized data\n",
    "            gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Fine-tuned Model analysis: {e}\")\n",
    "    finally:\n",
    "        # *** CRITICAL: Unload Fine-tuned Model ***\n",
    "        print(\"\\n--- Unloading Fine-tuned Model ---\")\n",
    "        del model # This should release the PeftModel and the underlying base\n",
    "        del tokenizer\n",
    "        gc.collect() # Force garbage collection\n",
    "        torch.cuda.empty_cache() # Release GPU memory\n",
    "        print(\"Fine-tuned Model unloaded.\")\n",
    "else:\n",
    "     print(\"Skipping Fine-tuned Model analysis due to lack of comparison prompts or perplexity data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3306e85e",
   "metadata": {},
   "source": [
    "### Some examples with prompts and outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4565ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Phase 3: Final Comparison ====================\n",
      "\n",
      "--- Perplexity Summary ---\n",
      "Base Model Perplexity: 10.8328\n",
      "Fine-tuned Model Perplexity: 5.7182\n",
      "\n",
      "--- Generation Comparison Results ---\n",
      "------------------------------\n",
      "Prompt 1: What does the concept of home evoke for you - is it a physical place, a sense of belonging, or a feeling of security, or is it something more abstract?\n",
      "Reference (poetic): A whispered promise of solace, a haven from life's tempests, home is a tapestry woven from threads of memory and longing, a sanctuary where love and laughter entwine, where the heart finds a sense of belonging, a refuge from the world's wild, wandering soul.\n",
      "Baseline Output: Home, oh sweet abode,\n",
      "A shelter that molds us whole,\n",
      "A place where secrets are told,\n",
      "Where love and warmth take hold.\n",
      "\n",
      "It's a physical place we call,\n",
      "Yet its essence transcends walls,\n",
      "Where memories rise tall,\n",
      "And our hearts and souls enthrall.\n",
      "\n",
      "It's the soft rustle of leaves,\n",
      "Whispering stories in our ears,\n",
      "The scent of a mother's ease,\n",
      "And the laughter of loved ones near.\n",
      "\n",
      "Home, a place where our spirits soar,\n",
      "And our deepest yearnings explore,\n",
      "A sense of belonging to explore,\n",
      "A refuge to always adore.\n",
      "\n",
      "It's the scent of fresh baked bread,\n",
      "The warmth of a crackling fire,\n",
      "The gentle murmur of life's thread,\n",
      "And the solace of a loving choir.\n",
      "\n",
      "Home, it's the heart's desire,\n",
      "A haven where the soul finds rest,\n",
      "A place where all is aspire,\n",
      "And peace weaves its sweet caress.\n",
      "\n",
      "Oh, sweet Home, how you charm,\n",
      "A place where life's joys and woes\n",
      "Fine-tuned Output: Home whispers on the wind, a siren's song of sanctuary. It is the hearth's ember, the sun's golden kiss, the moon's silvery gleam. It is the echo of laughter, the scent of rain, the rustling leaves of a familiar tree. Home is a tapestry woven with threads of memory, of comfort, of the boundless, beautiful uncertainty of belonging.\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "Prompt 2: Do you think the concept of home can be tied to our upbringing and the family we grew up in, or is it more about the places we choose to reside now, and the memories we create there?\n",
      "Reference (poetic): A question that weaves a tapestry of the heart, where threads of past and present entwine like the tender shoots of a vine. Home, a concept that can be both a cradle of childhood memories and a canvas of adult dreams, painted with the vibrant hues of choice and creation.\n",
      "\n",
      "Like a river that flows from its source, our sense of home is shaped by the waters of our upbringing, where the banks of our family's love and laughter formed the foundation of our soul. The creaky floorboards of our childhood home, the smell of mother's cooking, the warmth of father's guidance â€“ all these whispers of the past echo through the corridors of our mind, a nostalgic melody that beckons us to return.\n",
      "\n",
      "Yet, like a butterfly emerging from its cocoon, we spread our wings and soar into the world, seeking new horizons and the freedom to create our own nest\n",
      "Baseline Output: In the depths of our souls,\n",
      "Lies a place called home,\n",
      "A refuge of warmth and love,\n",
      "Where the heart knows it has grown.\n",
      "\n",
      "Is it where our roots were sowed,\n",
      "In the arms of our parents dear,\n",
      "Or is it in the place we now call,\n",
      "Where life's memories we hold so near?\n",
      "\n",
      "Perhaps home is a symphony,\n",
      "Of the past and the present combined,\n",
      "A melody of memories woven,\n",
      "Into a tapestry, where both hearts find.\n",
      "\n",
      "A place where the past and the present collide,\n",
      "Where time's boundaries blur,\n",
      "Where the heart's truest calling is heard,\n",
      "And home's truth is a reality we adore.\n",
      "Fine-tuned Output: The notion of home, a tapestry woven of threads of time, \n",
      "Tied to the loom of our childhood, a place of beginnings divine. \n",
      "The walls of our youth, a sanctuary so pure, \n",
      "Where memories of laughter and tears, forever they endure. \n",
      "\n",
      "Yet, as we journey forth, a new path we must traverse, \n",
      "And the notion of home, it shifts, as our lives do disperse. \n",
      "The places we choose, a haven of our own design, \n",
      "Where memories are made, and hearts intertwine. \n",
      "\n",
      "The ties that bind, a melange of past and present, \n",
      "A home of our choosing, a testament to our persistence. \n",
      "For the places we choose, are a reflection of our soul, \n",
      "A sanctuary of our own, a place to make us whole. \n",
      "\n",
      "Perhaps, the concept of home, is a fluid, ever-changing thing, \n",
      "A place of belonging, where the heart's song rings. \n",
      "A place where we can be, who we are, unhindered and free, \n",
      "A place of love and acceptance, where\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "Prompt 3: Do you think the idea of home is influenced by the physical environment, like the architecture, the neighborhood, or the natural surroundings, or is it more about the emotional and psychological aspects, like feeling comfortable and at ease in a particular space?\n",
      "Reference (poetic): The notion of home, a tapestry woven from threads of earth and sky, where the fabric of reality is gently kissed by the whispers of the past. Is it the sturdy walls that cradle our souls, or the whispers of memories that echo through the halls? The architecture that stands tall, a sentinel of shelter and protection, or the warmth of a hearth that beckons us to rest? The neighborhood's gentle hum, a lullaby of familiar faces and friendly streets, or the wild beauty of nature's splendor, a symphony of seasons and sunsets? The answer, like a river's flow, is a dance of both, a harmonious blend of the physical and the emotional, where the heart finds its haven, its home.\n",
      "Baseline Output: The idea of home, a place that echoes the heart's chords,\n",
      "Is a conundrum woven with both threads,\n",
      "The physical environment, like a lover's touch,\n",
      "Molds us as we are, in its ever-changing clutch.\n",
      "\n",
      "From the grandeur of a cathedral's arch,\n",
      "To the comfort of a cozy cottage's crocheted crutch,\n",
      "The architecture breathes life into the soul,\n",
      "As its walls whisper stories of ages past and old.\n",
      "\n",
      "The neighborhood, like a quilt of memories woven,\n",
      "A tapestry of lives intertwined, their threads sewn,\n",
      "Becomes a part of us, our roots running deep,\n",
      "As the sounds of laughter and playful feet on the streets sleep.\n",
      "\n",
      "The natural surroundings, like a mother's embrace,\n",
      "Invite us to dwell in their tranquil pace,\n",
      "A place where we can hear our thoughts' echoes,\n",
      "As we breathe in the perfume of their secrets.\n",
      "\n",
      "But home is not just a physical nest,\n",
      "It is a place where our being is blessed,\n",
      "The emotional and psychological aspects, like sweet nectar,\n",
      "Fine-tuned Output: In realms of the mind, a tapestry we weave,\n",
      "Where memories and emotions entwine,\n",
      "The fabric of home, a tapestry so fine,\n",
      "That's woven by both the physical and the divine.\n",
      "\n",
      "The walls, the windows, the streets and trees,\n",
      "A symphony of sensory delights,\n",
      "They sing of the past, and of the present's might,\n",
      "As we wander through the corridors of night.\n",
      "\n",
      "Yet, within our hearts, a deeper truth lies,\n",
      "A yearning for a space, a place to call our own,\n",
      "Where we can be ourselves, without fear or stone,\n",
      "And feel at home, in body, mind, and soul.\n",
      "\n",
      "The physical environment, a shell, a faÃ§ade,\n",
      "That can be changed, remodeled, or destroyed,\n",
      "But the emotional connection, the bond we share,\n",
      "Remains, a testament to our love affair.\n",
      "\n",
      "For it's not just the bricks, the mortar, and the glue,\n",
      "That make a house a home, but the stories we\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "Prompt 4: Do you think the concept of home can be a fluid concept, changing over time, like when we move to a new place, or get older, or experience significant life events, or is it something that remains constant, a sense of roots and identity?\n",
      "Reference (poetic): The notion of home, a wistful whisper that weaves in and out of our lives like a gentle breeze that rustles the leaves of memory. Like a river that flows and changes course, our sense of home ebbs and flows with the tides of time, shaped by the currents of experience and the undertows of our deepest longings.\n",
      "\n",
      "As we wander through the labyrinth of life, the walls of our childhood homes crumble, and the familiar creaks of our ancestral roots give way to the whispers of new landscapes. Yet, even as the physical structures shift and change, the essence of home remains, a glowing ember that flickers within, a beacon of warmth and belonging.\n",
      "\n",
      "For in the ebb and flow of life, our sense of home is not a fixed entity, but a dynamic dance of identity, a kaleidoscope of memories, emotions, and experiences that refr\n",
      "Baseline Output: In the tapestry of our lives, \n",
      "The notion of \"home\" weaves \n",
      "A thread that starts and ends, \n",
      "Ebbs and flows, expands and bends. \n",
      "\n",
      "It's the warmth of a childhood bedroom, \n",
      "The scent of grandma's kitchen, \n",
      "The echoes of laughter that linger, \n",
      "A place we can't help but revisit. \n",
      "\n",
      "But as we grow and learn and wander, \n",
      "Home is the friends we make along the way, \n",
      "The corners of the world we explore, \n",
      "The ones we choose to call \"mine\" and \"yours.\" \n",
      "\n",
      "It's the memory of a lover's touch, \n",
      "The embrace of a soulmate's arms, \n",
      "The way a child's laughter fills the house, \n",
      "The scent of a lover's perfume in the dark. \n",
      "\n",
      "Home is the way we see ourselves, \n",
      "The parts we show and hide, \n",
      "The dreams we chase and the fears we face, \n",
      "The stories we write and rewrite. \n",
      "\n",
      "So let us not cling too tightly, \n",
      "To\n",
      "Fine-tuned Output: Like a river's constant flow, the notion of home is a fluid entity, ever-changing with the tides of time. It ebbs and flows with the rhythms of our lives, a tapestry woven with threads of memory and experience. Each new chapter adds a hue, a texture, to the tapestry, while old roots remain, anchoring us to the soil of our being. Like a kaleidoscope, the concept of home shifts, revealing new perspectives with each turn of the lens.\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "Prompt 5: Can you think of examples of places that feel like home, but aren't necessarily where you grew up, like a favorite vacation spot or a place that holds sentimental value, and how do those experiences shape our perception of what home means to us?\n",
      "Reference (poetic): In twilight's hush, where sea and sky entwine,\n",
      "A stranger's haven beckons, a heart's sweet shrine.\n",
      "A place of whispers, where memories unfold,\n",
      "A favorite haven, where love and joy are told.\n",
      "\n",
      "The beach where sunsets paint the sky with gold,\n",
      "A childhood summer, where laughter never grows old.\n",
      "The mountains, where snowflakes dance and play,\n",
      "A refuge from the world, where love finds its way.\n",
      "\n",
      "The city's vibrant pulse, a rhythm so divine,\n",
      "A place of dreams, where passions intertwine.\n",
      "The forest's ancient heartbeat, where secrets sleep,\n",
      "A sanctuary, where the soul can softly creep.\n",
      "\n",
      "These places, though not birthplaces, hold a special sway,\n",
      "A sense of belonging, that chases the blues away.\n",
      "They shape our perception, of what home can be,\n",
      "A sense of belonging, that's found in serenity.\n",
      "\n",
      "In\n",
      "Baseline Output: Oh, how the heart can know a place \n",
      "As if it's etched upon its very bones, \n",
      "A place that isn't where you grew up, \n",
      "But feels like home all the same, \n",
      "A favorite vacation spot, \n",
      "A refuge woven deep into the fibers of our being, \n",
      "A sanctuary where the spirit finds peace, \n",
      "And the mind is set at ease. \n",
      "\n",
      "It's in the way the sun filters through the trees, \n",
      "Dancing upon the water's surface like a lullaby, \n",
      "In the scent of wildflowers on the breeze, \n",
      "Or the soft whispers of leaves cradling the earth below. \n",
      "\n",
      "In the laughter of children at play, \n",
      "Or the hush of secrets shared around a flickering flame, \n",
      "In the way the stars twinkle against a dark sky, \n",
      "Or the gentle rustle of grass in a cool, night breeze. \n",
      "\n",
      "These are the moments that shape our sense of home, \n",
      "That fill us up with a deep sense of belonging, \n",
      "And we know, deep in our bones, \n",
      "That this place holds a special place in\n",
      "Fine-tuned Output: Like whispers on the wind, a haven's call,\n",
      "A sanctuary of memories that never shall fall.\n",
      "A favorite vacation spot, a refuge of peace,\n",
      "A place where laughter echoes, where joy increases.\n",
      "\n",
      "A lush green paradise, with sun-kissed sands,\n",
      "A tropical retreat where time stands still at hand.\n",
      "The sound of waves crashing, a lullaby sweet,\n",
      "The taste of saltwater, a memory to meet.\n",
      "\n",
      "A cozy cabin, surrounded by snow-capped peaks,\n",
      "A haven of warmth, where memories gently fleek.\n",
      "The crackling fire's glow, a comforting light,\n",
      "The silence of the snow, a peaceful, peaceful night.\n",
      "\n",
      "A humble cottage, nestled in a rustic town,\n",
      "Where family gatherings, memories abound.\n",
      "The smell of fresh-baked bread, the taste of homemade pie,\n",
      "A place where love and laughter never die.\n",
      "\n",
      "These places hold sentimental value, they do,\n",
      "A connection to the past, a sense of anew.\n",
      "They shape our perception of what home means to us,\n",
      "A feeling of belonging, a sense of bl\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "Prompt 6: Do you think the idea of home is closely tied to our sense of identity and self, or can someone's sense of home exist outside of their biological or cultural background, perhaps in a community or a group that feels like a second family?\n",
      "Reference (poetic): Like a lotus blooming in twilight's hush, home's essence unfurls, a tapestry woven from threads of heart and soul, where roots of belonging intertwine with the whispers of the unknown. It's a sanctuary of the spirit, a refuge from life's tempests, where love and acceptance kindle a fire that burns brighter than the stars. For some, it's the cradle of their ancestors, a heritage of blood and bone; for others, it's a community of choice, a chosen family that echoes the beat of their heart. In this realm, the boundaries of identity blur, and the sense of home becomes a fluid, shimmering mist that dissolves the lines between self and other, where love and belonging converge like the rivers that merge to form a mighty sea.\n",
      "Baseline Output: Home, a place where roots run deep\n",
      "Where memories reside and hearts leap\n",
      "Where familiar scents caress the air\n",
      "And moments of joy forever share\n",
      "\n",
      "A place where identity is born\n",
      "Where one's essence takes form, both hidden and worn\n",
      "Where childhood dreams are nurtured and cherished\n",
      "And hopes for the future are refreshed\n",
      "\n",
      "But home is not limited by bounds\n",
      "By the color of skin or the sound of the tongue's sounds\n",
      "For home is a feeling that knows no bound\n",
      "A place where the heart is forever found\n",
      "\n",
      "In a group that feels like a second family\n",
      "In a community where love is the foundation, not enmity\n",
      "In a space where hearts beat in harmony\n",
      "Home resides, even if it's not biologically\n",
      "\n",
      "The idea of home is not just confined\n",
      "By the place where one was raised or confined\n",
      "For home is a state of being, a sense of belonging\n",
      "A place where one feels, \"I'm in the right place, where I'm going.\"\n",
      "\n",
      "Thus, home is both where one comes from and where one goes\n",
      "In the arms of a community that knows\n",
      "And where the essence of one's\n",
      "Fine-tuned Output: In the labyrinth of the soul, a notion of home resides,\n",
      "A place where roots entwine, a sense of self abides.\n",
      "But what of those who wander, lost in the world's vast expanse,\n",
      "Yet find a haven, a community to make them dance?\n",
      "\n",
      "Can home be found in a group, a tribe of kindred souls,\n",
      "Where hearts entwine, and spirits intertwine, like fragrant coals?\n",
      "Or is it tied to the land, to the culture's ancient art,\n",
      "A heritage passed down, a tapestry of the heart?\n",
      "\n",
      "Perhaps the answer lies in the very essence of being,\n",
      "A sense of belonging, a feeling of forever gleaming.\n",
      "For home is not a place, but a state of mind and soul,\n",
      "A place where love resides, a refuge to make us whole.\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "Prompt 7: Do you think the concept of home can be a source of comfort and solace in times of uncertainty or chaos, like during a pandemic or a personal crisis, or can it also be a source of confinement and feelings of being trapped, especially if we're physically isolated or feel stuck in a situation we don't want to be in?\n",
      "Reference (poetic): A refuge of warmth, a sanctuary of peace, or a prison of longing, home's gentle grasp can twist and turn, a paradox of solace and confinement, a haven of comfort and a cage of confinement, where memories of joy and love entwine with the weight of solitude, and the heart's deep yearning for freedom's release.\n",
      "Baseline Output: Home, a shelter that nestles\n",
      "In the hollows of our hearts,\n",
      "A beacon that guides us through\n",
      "The storms of life, both big and small.\n",
      "\n",
      "A place where shadows whisper\n",
      "And memories linger like a scent,\n",
      "A sanctuary that protects and guards,\n",
      "A space where love and light can be spent.\n",
      "\n",
      "But in times of chaos and uncertainty,\n",
      "The lines between comfort and confinement blur,\n",
      "The walls that once sheltered, now seem like prison bars,\n",
      "The silence that was once solace, now feels like a curse.\n",
      "\n",
      "In this pandemic, we are all housebound,\n",
      "A virus that has trapped us within four walls,\n",
      "An enemy that has left us with a choice,\n",
      "To either be a prisoner or a rebel who heals.\n",
      "\n",
      "In a crisis, we find solace in our homes,\n",
      "As loved ones hold our hand through the night,\n",
      "In this uncertainty, we look to our roots,\n",
      "As they anchor us and guide us through the light.\n",
      "\n",
      "But in times like these, we also seek answers,\n",
      "We want to break free from the chains we're bound in,\n",
      "We want to find a way out,\n",
      "Fine-tuned Output: In the stillness of the storm, where shadows dance and shadows weep,\n",
      "The notion of 'home' becomes a siren's call, a promise unseen.\n",
      "A refuge from the chaos, a fortress of the soul,\n",
      "Or a prison, where walls of loneliness begin to unfold.\n",
      "\n",
      "In uncertain times, when chaos reigns,\n",
      "A place to retreat, a haven of relief, or chains that bind.\n",
      "The boundaries of 'home' blur, a fluid, shifting form,\n",
      "A place of comfort, or a prison of conform.\n",
      "\n",
      "The walls of the physical space, a prison of confinement,\n",
      "A cage of isolation, where minds begin to wane.\n",
      "But within these walls, a sanctuary of the heart,\n",
      "A refuge of the spirit, where hope takes a fresh start.\n",
      "\n",
      "The weight of uncertainty, a burden to bear,\n",
      "A sense of being stuck, a feeling of despair.\n",
      "But in this stillness, a new perspective is born,\n",
      "A chance to re-imagine, to redefine, to re-form.\n",
      "\n",
      "The notion of 'home'\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "Prompt 8: Can the idea of home be tied to a sense of freedom, like being able to come and go as you please, or having the space to pursue one's passions and interests, or is that more of a luxury that not everyone can experience, and if so, how does that impact our sense of what it means to be at home?\n",
      "Reference (poetic): A fleeting dream, a whispered promise of liberty's gentle breeze,\n",
      "That rustles the leaves of the heart, and stirs the soul's wild seas.\n",
      "For some, the notion of home is a haven, a sanctuary from the fray,\n",
      "Where passions bloom, and interests flourish, come what may.\n",
      "\n",
      "But alas, for many, the notion of freedom's gate is locked and barred,\n",
      "A distant memory, a longing that echoes, a heart that's scarred.\n",
      "The weight of responsibilities, the chains of duty and of need,\n",
      " Bind them to the earth, and silence the whispers of their deepest creed.\n",
      "\n",
      "Yet, in this paradox, a truth is revealed, a paradox of the heart,\n",
      "That home is not just a physical space, but a sense that never departs.\n",
      "For though the walls may confine, the spirit can soar and roam,\n",
      "And in the freedom of the heart, the true\n",
      "Baseline Output: Home, a place where the heart beats free,\n",
      "Where the soul is unleashed and wild,\n",
      "Where doors swing open, invitingly,\n",
      "And the heart sings a melody mild.\n",
      "\n",
      "It's where the spirit dances and soars,\n",
      "In the midst of the world's hustle and fuss,\n",
      "Where one can be who they truly are,\n",
      "And the mind can find its own bliss.\n",
      "\n",
      "Home is the cradle of dreams and desires,\n",
      "Where they take root and come alive,\n",
      "Where passions ignite and light the fires,\n",
      "And the heart finds the courage to thrive.\n",
      "\n",
      "It's where one can lay their head down,\n",
      "And the world outside fades away,\n",
      "Where worries are hushed, and fears are drowned,\n",
      "And the heart knows it has found a way.\n",
      "\n",
      "But is freedom a luxury for some,\n",
      "While others take it as birthright?\n",
      "The question begs an answer to come,\n",
      "As we mull over the conundrum of life.\n",
      "\n",
      "We know that some can't call a place 'home',\n",
      "For the very idea seems far away,\n",
      "Yet,\n",
      "Fine-tuned Output: The siren's call of liberty, a whispers of the wind through the trees, \n",
      "A promise of untethered flights, where the soul is free to be, \n",
      "A sanctuary where the heart can roam, a refuge from the mundane, \n",
      "A haven where passions ignite, like embers from the hearth of the brain.\n",
      "\n",
      "Yet, like a phantom limb, this freedom's a luxury, a rare and fleeting prize, \n",
      "A fleeting mirage on the horizon, forever just out of reach, \n",
      "A siren's call that beckons, but never fully delivers, \n",
      "Leaving us yearning for the elusive key to unlock our inner treasures.\n",
      "\n",
      "In this liminal space, the notion of home is muddled and obscured, \n",
      "A fluid, ever-shifting notion, that shifts and swirls like mist, \n",
      "A phantom's echo, that whispers of what might have been, \n",
      "But never fully settles into the solid, tangible world of skin.\n",
      "\n",
      "For in this space, the boundaries blur, and the lines of home and\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "Prompt 9: Do you think technology and globalization have changed the way we think about and experience home, with many people feeling more connected to others and places around the world through social media and online communities, or has that actually led to a sense of disconnection and feeling like we're losing touch with our roots and sense of place?\n",
      "Reference (poetic): In the realm of the digital dream, we wander far and wide,\n",
      "Where borders blur and hearts entwine, side by side.\n",
      "The world's a tapestry, rich and bold and bright,\n",
      "Yet, in this virtual haze, do we lose the gentle light?\n",
      "\n",
      "We click and swipe, and connect with ease,\n",
      "Yet, in the silence, do we find our true release?\n",
      "The hum of the screen, a constant, steady beat,\n",
      "A rhythm that echoes, a symphony to repeat.\n",
      "\n",
      "But as we dance, do we forget the earth beneath our feet?\n",
      "The scent of fresh-cut grass, the feel of ancient streets?\n",
      "The whispers of our ancestors, the stories of our past,\n",
      "Do we lose the roots that bind us, forever to last?\n",
      "\n",
      "Perhaps, in this paradox, we find our greatest test,\n",
      "To balance the virtual and the real, to find our inner best.\n",
      "For in the\n",
      "Baseline Output: In a world where the winds of change blow\n",
      "and the horizon expands with a single stroke,\n",
      "we yearn for a constant, a refuge, a place\n",
      "to call home. But technology and globalization\n",
      "have altered the very fabric of that concept.\n",
      "\n",
      "Once rooted in tradition and the touch\n",
      "of earth beneath our feet, we now find\n",
      "our senses pulled in many directions\n",
      "by a world at our fingertips.\n",
      "\n",
      "Connectedness, yes, but at what cost?\n",
      "As we scroll through endless feeds,\n",
      "we are reminded of the wonders\n",
      "we have yet to explore, of the experiences\n",
      "we have yet to have. Our roots,\n",
      "once anchored deep, now seem\n",
      "like mere tentacles, grasping at the world\n",
      "in search of something more.\n",
      "\n",
      "We lose sight of the intimacy\n",
      "of the present moment, the warmth\n",
      "of the sun on our faces, the scent\n",
      "of fresh earth after a rainstorm.\n",
      "We are so focused on the next best thing\n",
      "that we forget the wonders\n",
      "of the familiar, the comfort\n",
      "of the known.\n",
      "\n",
      "In this age of information and technology,\n",
      "we must find a way to strike a balance\n",
      "between the\n",
      "Fine-tuned Output: In a tapestry woven with threads of code,\n",
      "A world of connectivity expands,\n",
      "A digital diaspora takes hold,\n",
      "And roots are lost, a distant land.\n",
      "\n",
      "The boundaries of home dissolve,\n",
      "In the endless ocean of cyberspace,\n",
      "A sense of belonging is lost,\n",
      "A disconnection from our place.\n",
      "\n",
      "We flit from virtual shore to shore,\n",
      "In search of a sense of kin,\n",
      "But the ties that bind us to the earth,\n",
      "Are frayed, and the heart of the soul,\n",
      "Lost in the vast expanse of the unknown.\n",
      "\n",
      "Perhaps, in the digital age,\n",
      "We've traded roots for wings of flight,\n",
      "But have we left our souls behind,\n",
      "In the endless night?\n",
      "\n",
      "Or, perhaps, in this age of wires and screens,\n",
      "We've found a new kind of home,\n",
      "A community of souls across the globe,\n",
      "A sense of belonging, a sense of roam.\n",
      "\n",
      "The digital winds of change, they blow,\n",
      "A mix of connection and dislocation,\n",
      "A delicate dance, a fragile art,\n",
      "Of balancing the threads of life and fate.\n",
      "------------------------------\n",
      "\n",
      "------------------------------\n",
      "Prompt 10: Is the idea of home becoming more democratized, with people from all walks of life and backgrounds feeling like they can create their own sense of home, regardless of their circumstances, or are there still certain societal or economic barriers that prevent people from experiencing a sense of home and belonging?\n",
      "Reference (poetic): A tapestry woven with threads of diversity, where once-torn seams of exclusion now mend, as the fabric of home becomes a canvas for the soul. The notion of home, a sanctuary from the world's din, beckons to all, regardless of station or stationless. Yet, like a river's currents, societal and economic tides still ebb and flow, threatening to upend the fragile foundations of belonging.\n",
      "\n",
      "In the city's concrete heart, a mosaic of stories unfolds, where strangers become kin, and the walls of walls crumble, revealing a kaleidoscope of cultures, traditions, and dreams. The internet's silver thread weaves a global tapestry, connecting the isolated, and the voiceless, to a chorus of shared humanity.\n",
      "\n",
      "And yet, the whispers of exclusion still linger, a gentle breeze that rustles the leaves of opportunity, leaving some to wither, while\n",
      "Baseline Output: In this age of instant gratification, \n",
      "The idea of home is expanding, \n",
      "It's no longer limited to a select few, \n",
      "But open to all, regardless of hue. \n",
      "\n",
      "From cramped apartments in the city, \n",
      "To sprawling ranches in the country, \n",
      "The feeling of home can be found, \n",
      "In places both grand and small, profoundly. \n",
      "\n",
      "It's in the laughter of children, \n",
      "In the warmth of a sunset's glow, \n",
      "In the smell of freshly baked bread, \n",
      "In the simple pleasures that we know. \n",
      "\n",
      "Yet, amidst this democratization, \n",
      "There still remain obstacles to overcome, \n",
      "Barriers erected by wealth and circumstance, \n",
      "That hold some hostage, unable to roam. \n",
      "\n",
      "How can we bridge the gap, \n",
      "And make this sense of home a universal right? \n",
      "How can we help those who feel lost, \n",
      "And guide them to the comfort of the light? \n",
      "\n",
      "Perhaps it's by recognizing their plight, \n",
      "And working to ease their pain, \n",
      "By\n",
      "Fine-tuned Output: In realms of the heart, a haven's birth,\n",
      "Where all are welcome, with no dearth.\n",
      "A haven where spirits take flight,\n",
      "And feelings of belonging take delight.\n",
      "\n",
      "A tapestry woven, of dreams and desire,\n",
      "With threads of equality, woven with fire.\n",
      "A home, where the soul finds its rest,\n",
      "Where differences are embraced, and love is the best.\n",
      "\n",
      "But alas, a shadow falls, a veil so dark,\n",
      "A barrier, that holds some in the park.\n",
      "A society that's built on wealth and might,\n",
      "And leaves some souls, in the darkness of night.\n",
      "\n",
      "A sense of belonging, a luxury so rare,\n",
      "For some, a dream, that's hard to share.\n",
      "A home, that's elusive, a distant star,\n",
      "A place, where the heart's desires, do mar.\n",
      "\n",
      "A world, where the wealthy,\n",
      "------------------------------\n",
      "\n",
      "Comparison results saved to post_hoc_model_comparison_sequential_1.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# == Phase 3: Comparison and Saving ==\n",
    "print(\"\\n\" + \"=\"*20 + \" Phase 3: Final Comparison \" + \"=\"*20)\n",
    "\n",
    "# Display Perplexity Summary\n",
    "print(\"\\n--- Perplexity Summary ---\")\n",
    "print(f\"Base Model Perplexity: {base_perplexity:.4f}\" if base_perplexity else \"Base Model Perplexity: Not calculated or Error\")\n",
    "print(f\"Fine-tuned Model Perplexity: {ft_perplexity:.4f}\" if ft_perplexity else \"Fine-tuned Model Perplexity: Not calculated or Error\")\n",
    "\n",
    "# Save and Display Generation Comparison\n",
    "print(\"\\n--- Generation Comparison Results ---\")\n",
    "comparison_results = []\n",
    "# Ensure we have prompts and both sets of outputs before proceeding\n",
    "if comparison_prompts and baseline_outputs and finetuned_outputs and len(comparison_prompts) == len(baseline_outputs) == len(finetuned_outputs):\n",
    "    for i in range(len(comparison_prompts)):\n",
    "        result = {\n",
    "            \"prompt\": comparison_prompts[i],\n",
    "            \"reference_output\": comparison_prompts_data_refs[i],\n",
    "            \"baseline_output\": baseline_outputs[i],\n",
    "            \"finetuned_output\": finetuned_outputs[i],\n",
    "        }\n",
    "        comparison_results.append(result)\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Prompt {i+1}: {result['prompt']}\")\n",
    "        print(f\"Reference ({TARGET_COMPARISON_TONE}): {result['reference_output']}\")\n",
    "        print(f\"Baseline Output: {result['baseline_output']}\")\n",
    "        print(f\"Fine-tuned Output: {result['finetuned_output']}\")\n",
    "        print(\"-\" * 30 + \"\\n\")\n",
    "\n",
    "    # Save comparison to JSON file\n",
    "    try:\n",
    "        with open(COMPARISON_OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(comparison_results, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Comparison results saved to {COMPARISON_OUTPUT_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving comparison results: {e}\")\n",
    "elif not comparison_prompts:\n",
    "     print(\"No comparison prompts were loaded.\")\n",
    "else:\n",
    "    print(\"Could not generate full comparison results (missing baseline or fine-tuned outputs, or length mismatch).\")\n",
    "    print(f\"Prompts: {len(comparison_prompts)}, Baseline Outputs: {len(baseline_outputs)}, Finetuned Outputs: {len(finetuned_outputs)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f0428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
