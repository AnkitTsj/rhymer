{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67dc99f2",
   "metadata": {},
   "source": [
    "### Unsloth Fine-tuning Configuration\n",
    "This code sets up a fine-tuning pipeline for a language model using the Unsloth library, which optimizes transformer models for faster training and inference. The model will be trained on conversation data to generate responses in different tones.\n",
    "Configuration Parameters\n",
    "\n",
    "pythonMODEL_NAME = \"unsloth/zephyr-sft-bnb-4bit\"\n",
    "\n",
    "OUTPUT_DIR = \"./results_optimized_all_tones_dynamic\" \n",
    "\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "\n",
    "LoRA Parameters\n",
    "\n",
    "Rank (r): 16\n",
    "Alpha: 16\n",
    "Dropout: 0\n",
    "Target Modules: Attention layers and MLP components\n",
    "\n",
    "Training Parameters\n",
    "\n",
    "Batch Size: 2 per device\n",
    "\n",
    "Gradient Accumulation: 4 steps\n",
    "\n",
    "Learning Rate: 2e-4\n",
    "\n",
    "Epochs: 3\n",
    "\n",
    "Warmup Steps: 10\n",
    "\n",
    "Optimizer: AdamW (8-bit)\n",
    "\n",
    "Weight Decay: 0.01\n",
    "\n",
    "Scheduler: Linear\n",
    "\n",
    "Random Seed: 3407\n",
    "\n",
    "Response Types\n",
    "The model will be trained to generate content in four distinct tones:\n",
    "\n",
    "Mixed (general)\n",
    "Love\n",
    "Philosophical\n",
    "Poetic\n",
    "\n",
    "Evaluation Strategy\n",
    "\n",
    "Evaluation every 100 steps ---\n",
    "\n",
    "Model checkpoint saving every 100 steps ---\n",
    "\n",
    "Comparison sampling with 5 examples ---\n",
    "\n",
    "Target comparison tone: poetic ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5d029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Rtemp\\Rtemp\\ipykernel_9780\\3840710422.py:3: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel # Using Unsloth's class\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "WARNING:tensorflow:From c:\\Users\\Ankit Kumar\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import TrainingArguments, EvalPrediction\n",
    "from peft import PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from unsloth import is_bfloat16_supported\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import gc \n",
    "\n",
    "MODEL_NAME = \"unsloth/zephyr-sft-bnb-4bit\"\n",
    "OUTPUT_DIR = \"./results_optimized_all_tones_dynamic\" \n",
    "DATA_TRAIN_PATH = \"E:/papers-i-implement/poet/data/conversation_data.json\"\n",
    "DATA_TEST_PATH = \"E:/papers-i-implement/poet/data/conversation_data_test.json\"\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "RESPONSE_TYPES = [\"mixed\", \"love\", \"philosophical\", \"poetic\"]\n",
    "\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 2\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "WARMUP_STEPS = 10\n",
    "LOGGING_STEPS = 50\n",
    "EVAL_STEPS = 100\n",
    "SAVE_STEPS = 100\n",
    "OPTIMIZER = \"adamw_8bit\"\n",
    "WEIGHT_DECAY = 0.01\n",
    "LR_SCHEDULER_TYPE = \"linear\"\n",
    "SEED = 3407\n",
    "EVALUATION_STRATEGY = \"steps\"\n",
    "\n",
    "NUM_COMPARISON_SAMPLES = 5\n",
    "COMPARISON_OUTPUT_FILE = \"model_comparison_outputs_dynamic.json\"\n",
    "TARGET_COMPARISON_TONE = \"poetic\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c06709",
   "metadata": {},
   "source": [
    "### Model and Tokenizer Initialization\n",
    "This section handles loading the base model and tokenizer, setting up special tokens, and configuring the processing environment for data preparation.\n",
    "Processing Configuration\n",
    "\n",
    "pythonNUM_PROC = 1  # Sequential processing\n",
    "\n",
    "Model Loading\n",
    "The code initializes the pre-trained Zephyr model using Unsloth's optimization framework:\n",
    "\n",
    "Uses 4-bit quantization for memory efficiency\n",
    "\n",
    "Configures for maximum sequence length of 1024 tokens\n",
    "\n",
    "Automatically maps model across available devices\n",
    "\n",
    "Special Token Setup\n",
    "The code includes safeguards to ensure essential special tokens are properly defined:\n",
    "\n",
    "EOS Token Check:\n",
    "\n",
    "Adds <|endoftext|> if no end-of-sequence token exists\n",
    "\n",
    "Critical for proper sequence termination\n",
    "\n",
    "\n",
    "PAD Token Configuration:\n",
    "\n",
    "Falls back to using EOS token for padding if no pad token exists\n",
    "\n",
    "Updates model configuration to recognize the pad token ID\n",
    "\n",
    "\n",
    "\n",
    "This initialization process ensures the model is properly configured before fine-tuning begins, with appropriate tokenization boundaries and memory-efficient loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612c5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 processes for data loading/processing (forced sequential).\n",
      "Loading base model: unsloth/zephyr-sft-bnb-4bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ankit Kumar\\anaconda3\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Mistral patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3050 6GB Laptop GPU. Num GPUs = 1. Max memory: 6.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Base model and tokenizer loaded.\n",
      "EOS token: </s>, PAD token: <unk>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data Loading Config\n",
    "NUM_PROC = 1\n",
    "print(f\"Using {NUM_PROC} processes for data loading/processing (forced sequential).\")\n",
    "\n",
    "# --- Load Base Model and Tokenizer ---\n",
    "print(f\"Loading base model: {MODEL_NAME}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None, \n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\" \n",
    ")\n",
    "print(\"Base model and tokenizer loaded.\")\n",
    "\n",
    "# --- Set EOS and PAD Tokens ---\n",
    "if tokenizer.eos_token is None:\n",
    "    print(\"Warning: EOS token not found, adding '<|endoftext|>' as EOS token.\")\n",
    "    tokenizer.add_special_tokens({'eos_token': '<|endoftext|>'})\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Warning: PAD token not found. Setting PAD token to EOS token.\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    if hasattr(model, 'config') and hasattr(model.config, 'pad_token_id'):\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "print(f\"EOS token: {EOS_TOKEN}, PAD token: {tokenizer.pad_token}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff01451",
   "metadata": {},
   "source": [
    "### Baseline Evaluation Process\n",
    "This section handles the selection of test samples for model comparison and generates baseline outputs using the pre-fine-tuned model.\n",
    "Comparison Sample Selection\n",
    "\n",
    "pythonNUM_COMPARISON_SAMPLES = 5\n",
    "\n",
    "TARGET_COMPARISON_TONE = \"poetic\"\n",
    "\n",
    "The code loads a subset of test data to create a comparison benchmark:\n",
    "\n",
    "Extracts prompts and reference responses from the test dataset\n",
    "\n",
    "Specifically targets samples with the \"poetic\" tone for evaluation\n",
    "\n",
    "Implements error handling for robust data loading\n",
    "\n",
    "Baseline Generation\n",
    "Before fine-tuning begins, the code captures how the base model responds to the selected prompts:\n",
    "\n",
    "Prompt Formatting:\n",
    "\n",
    "Structures each prompt with explicit tone instructions\n",
    "Format: USER: {input text} TONE:poetic ASSISTANT:\n",
    "\n",
    "\n",
    "Generation Parameters:\n",
    "\n",
    "Maximum of 256 new tokens per response\n",
    "\n",
    "Uses sampling with temperature (top-k=50, top-p=0.9)\n",
    "\n",
    "Properly handles EOS and padding tokens\n",
    "\n",
    "\n",
    "Memory Management:\n",
    "\n",
    "Implements explicit tensor cleanup within the generation loop\n",
    "\n",
    "Forces garbage collection and CUDA cache clearing\n",
    "\n",
    "Designed to prevent memory issues on limited hardware\n",
    "\n",
    "\n",
    "\n",
    "This baseline will serve as a comparison point to measure improvement after fine-tuning, specifically focusing on the model's ability to generate poetic responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20edf330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw test data to select 5 comparison samples...\n",
      "Selected 5 prompts for comparison.\n",
      "\n",
      "--- Generating Baseline Outputs (Before Fine-tuning) ---\n",
      "Baseline Sample 1/5 Generated.\n",
      "Baseline Sample 2/5 Generated.\n",
      "Baseline Sample 3/5 Generated.\n",
      "Baseline Sample 4/5 Generated.\n",
      "Baseline Sample 5/5 Generated.\n",
      "--- Baseline Generation Complete ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Load Comparison Prompts (CPU) ---\n",
    "print(f\"Loading raw test data to select {NUM_COMPARISON_SAMPLES} comparison samples...\")\n",
    "comparison_prompts_data = []\n",
    "comparison_prompts = []\n",
    "try:\n",
    "    comparison_raw_data = load_dataset(\"json\", data_files=DATA_TEST_PATH, split=f\"train[:{NUM_COMPARISON_SAMPLES * 2}]\")\n",
    "    for example in comparison_raw_data:\n",
    "        if 'conversation' in example and 'input' in example['conversation']:\n",
    "             comparison_prompts_data.append({\n",
    "                 \"input\": example['conversation']['input'],\n",
    "                 \"reference_output\": example['conversation'].get('responses', {}).get(TARGET_COMPARISON_TONE, None)\n",
    "             })\n",
    "        if len(comparison_prompts_data) >= NUM_COMPARISON_SAMPLES:\n",
    "            break\n",
    "    comparison_prompts = [item['input'] for item in comparison_prompts_data]\n",
    "    print(f\"Selected {len(comparison_prompts)} prompts for comparison.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test data for comparison samples: {e}\")\n",
    "\n",
    "\n",
    "# --- Baseline Model Generation ---\n",
    "# This uses the currently loaded base model\n",
    "print(\"\\n--- Generating Baseline Outputs (Before Fine-tuning) ---\")\n",
    "baseline_outputs = []\n",
    "if comparison_prompts:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, prompt_text in enumerate(comparison_prompts):\n",
    "            formatted_prompt = f\"USER: {prompt_text}\\n TONE:{TARGET_COMPARISON_TONE} \\nASSISTANT: \"\n",
    "            # Use tokenizer directly for potentially better performance with FastTokenizer\n",
    "            inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH // 2).to(model.device) # Use model's current device\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **inputs, max_new_tokens=256, eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id, do_sample=True, top_k=50, top_p=0.9\n",
    "            )\n",
    "            output_text = tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "            baseline_outputs.append(output_text.strip())\n",
    "            print(f\"Baseline Sample {i+1}/{len(comparison_prompts)} Generated.\")\n",
    "            # Clean up tensors explicitly inside loop for safety on low VRAM\n",
    "            del inputs, outputs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipping baseline generation as no comparison prompts were loaded.\")\n",
    "print(\"--- Baseline Generation Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a7130",
   "metadata": {},
   "source": [
    "### Model Architecture and Data Processing\n",
    "This section configures the model for Parameter-Efficient Fine-Tuning (PEFT) and prepares the training datasets with multi-tone formatting.\n",
    "\n",
    "LoRA Configuration\n",
    "\n",
    "pythonLORA_R = 16\n",
    "\n",
    "LORA_ALPHA = 16\n",
    "\n",
    "LORA_DROPOUT = 0\n",
    "\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "The code applies Low-Rank Adaptation (LoRA) to the base model:\n",
    "\n",
    "Targets key projection matrices in both attention and MLP components\n",
    "Uses Unsloth's optimized gradient checkpointing for memory efficiency\n",
    "Maintains reproducibility with fixed random seed\n",
    "\n",
    "Dataset Preparation\n",
    "The code processes conversation data into instruction-following format across multiple response tones:\n",
    "\n",
    "Format Structure:\n",
    "\n",
    "USER: {input}\n",
    "\n",
    "TONE:{response_type}\n",
    "\n",
    "ASSISTANT: {response}{eos_token}\n",
    "\n",
    "Multi-Tone Processing:\n",
    "\n",
    "Creates separate datasets for each tone (mixed, love, philosophical, poetic)\n",
    "Filters out examples missing responses for any particular tone\n",
    "Combines datasets while preserving tone distribution\n",
    "\n",
    "\n",
    "Memory Management:\n",
    "\n",
    "Explicitly removes intermediate processing objects\n",
    "Forces garbage collection to free memory\n",
    "\n",
    "\n",
    "\n",
    "This approach enables the model to learn different response styles based on the explicit tone instruction, creating a dynamic response capability controlled through the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b9d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying PEFT (LoRA)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT applied. Model is now PEFT-enabled.\n",
      "\n",
      "Loading and preprocessing full datasets...\n",
      "Total Train dataset size: 92008\n",
      "Total Eval dataset size: 10400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Apply PEFT with LoRA ---\n",
    "# Applies LoRA adapters to the existing 'model' object\n",
    "print(\"\\nApplying PEFT (LoRA)...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model, \n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\", \n",
    "    random_state=SEED,\n",
    ")\n",
    "print(\"PEFT applied. Model is now PEFT-enabled.\")\n",
    "\n",
    "# --- Load and Preprocess Full Datasets (CPU) ---\n",
    "print(\"\\nLoading and preprocessing full datasets...\")\n",
    "# --- Define Preprocessing Function ---\n",
    "def format_prompt(example, response_type=\"mixed\"):\n",
    "    conv = example.get('conversation')\n",
    "    if not conv or 'input' not in conv or 'responses' not in conv or response_type not in conv['responses']: return {\"text\": None}\n",
    "    user_input = conv['input']\n",
    "    response = conv['responses'][response_type]\n",
    "    formatted_text = f\"USER: {user_input}\\n TONE:{response_type} \\nASSISTANT: {response}{tokenizer.eos_token}\" \n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# --- Preprocess and Combine Datasets ---\n",
    "all_train_datasets = []\n",
    "all_eval_datasets = []\n",
    "raw_train_data = load_dataset(\"json\", data_files=DATA_TRAIN_PATH, split=\"train\")\n",
    "raw_test_data = load_dataset(\"json\", data_files=DATA_TEST_PATH, split=\"train\")\n",
    "for r_type in RESPONSE_TYPES:\n",
    "    # Process Training Data\n",
    "    processed_train = raw_train_data.map(lambda x: format_prompt(x, r_type), num_proc=NUM_PROC, remove_columns=list(raw_train_data.features)).filter(lambda x: x['text'] is not None)\n",
    "    all_train_datasets.append(processed_train)\n",
    "    # Process Evaluation Data\n",
    "    processed_eval = raw_test_data.map(lambda x: format_prompt(x, r_type), num_proc=NUM_PROC, remove_columns=list(raw_test_data.features)).filter(lambda x: x['text'] is not None)\n",
    "    all_eval_datasets.append(processed_eval)\n",
    "train_dataset = concatenate_datasets(all_train_datasets).shuffle(seed=SEED)\n",
    "eval_dataset = concatenate_datasets(all_eval_datasets).shuffle(seed=SEED) # Shuffle eval too\n",
    "print(f\"Total Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Total Eval dataset size: {len(eval_dataset)}\")\n",
    "del raw_train_data, raw_test_data, all_train_datasets, all_eval_datasets, processed_train, processed_eval # Clean up raw data objects\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2670088",
   "metadata": {},
   "source": [
    "### Dataset Sampling and Validation\n",
    "This section implements a controlled dataset size reduction for faster experimentation and performs data quality validation.\n",
    "Dataset Size Control\n",
    "\n",
    "pythonNUM_TRAIN_SAMPLES = 10000\n",
    "\n",
    "NUM_TEST_SAMPLES = 3000\n",
    "\n",
    "The code deliberately limits the dataset size for rapid prototyping:\n",
    "\n",
    "Reduces training set to 10,000 examples\n",
    "Limits evaluation set to 3,000 examples\n",
    "\n",
    "Includes safeguards for datasets smaller than the requested size\n",
    "\n",
    "Data Quality Verification\n",
    "Critical validation steps ensure dataset integrity before training:\n",
    "\n",
    "Empty String Filtering:\n",
    "\n",
    "Removes any examples with null or empty text fields\n",
    "Reports dataset sizes before and after filtering\n",
    "Verifies datasets remain non-empty after cleaning\n",
    "\n",
    "\n",
    "Failure Protection:\n",
    "\n",
    "Includes explicit program termination if datasets become empty\n",
    "Suggests troubleshooting the data source or formatting function\n",
    "\n",
    "\n",
    "\n",
    "This controlled dataset reduction enables faster development cycles while maintaining enough examples to validate the model's ability to generate responses across different tones. The explicit verification steps prevent training failures due to data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf752b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---!!! WARNING: Selecting only 10000 samples for TRAIN and EVAL for quick testing! !!!---\n",
      "Using 10000 train samples for testing.\n",
      "Using 3000 eval samples for testing.\n",
      "\n",
      "Dataset size before filtering empty strings: 10000\n",
      "Dataset size after filtering empty strings: 10000\n",
      "Eval Dataset size before filtering empty strings: 3000\n",
      "Eval Dataset size after filtering empty strings: 3000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_TRAIN_SAMPLES = 10000\n",
    "NUM_TEST_SAMPLES = 3000\n",
    "print(f\"\\n---!!! WARNING: Selecting only {NUM_TRAIN_SAMPLES} samples for TRAIN and EVAL for quick testing! !!!---\")\n",
    "\n",
    "if len(train_dataset) >= NUM_TRAIN_SAMPLES:\n",
    "    # Use .select() to get the first N samples\n",
    "    train_dataset = train_dataset.select(range(NUM_TRAIN_SAMPLES))\n",
    "else:\n",
    "     print(f\"Warning: Train dataset has less than {NUM_TRAIN_SAMPLES} samples. Using all {len(train_dataset)} samples.\")\n",
    "\n",
    "if len(eval_dataset) >= NUM_TEST_SAMPLES:\n",
    "    eval_dataset = eval_dataset.select(range(NUM_TEST_SAMPLES))\n",
    "else:\n",
    "     print(f\"Warning: Eval dataset has less than {NUM_TEST_SAMPLES} samples. Using all {len(eval_dataset)} samples.\")\n",
    "\n",
    "print(f\"Using {len(train_dataset)} train samples for testing.\")\n",
    "print(f\"Using {len(eval_dataset)} eval samples for testing.\\n\")\n",
    "\n",
    "print(f\"Dataset size before filtering empty strings: {len(train_dataset)}\")\n",
    "train_dataset = train_dataset.filter(lambda example: example.get('text') is not None and len(example['text']) > 0)\n",
    "print(f\"Dataset size after filtering empty strings: {len(train_dataset)}\")\n",
    "# Repeat for eval_dataset \n",
    "print(f\"Eval Dataset size before filtering empty strings: {len(eval_dataset)}\")\n",
    "eval_dataset = eval_dataset.filter(lambda example: example.get('text') is not None and len(example['text']) > 0)\n",
    "print(f\"Eval Dataset size after filtering empty strings: {len(eval_dataset)}\")\n",
    "\n",
    "\n",
    "# Ensure datasets are not empty after filtering\n",
    "if len(train_dataset) == 0 or len(eval_dataset) == 0:\n",
    "    print(\"Error: Dataset became empty after filtering empty strings. Check data source or format_prompt.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b7c69",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "This section sets up the evaluation metrics and training parameters for the supervised fine-tuning process.\n",
    "Evaluation Metrics\n",
    "pythondef compute_metrics(p: EvalPrediction):\n",
    "    # Calculates perplexity from prediction logits and label IDs\n",
    "The code implements a custom evaluation function that:\n",
    "\n",
    "Computes cross-entropy loss on shifted prediction sequences\n",
    "Converts loss to perplexity (exp(loss)) for model quality assessment\n",
    "Includes explicit tensor cleanup to prevent memory leaks\n",
    "\n",
    "Training Setup\n",
    "The training configuration balances efficiency with performance:\n",
    "\n",
    "Training Parameters:\n",
    "\n",
    "3 epochs with linear learning rate schedule\n",
    "\n",
    "8-bit AdamW optimizer with weight decay\n",
    "\n",
    "Evaluation every 100 steps\n",
    "\n",
    "Model checkpoint saving based on evaluation performance\n",
    "\n",
    "\n",
    "Optimization Features:\n",
    "\n",
    "Gradient checkpointing to reduce memory usage\n",
    "\n",
    "Keeps only the 10 best checkpoints to manage storage\n",
    "\n",
    "Loads the best model at end based on lowest perplexity\n",
    "\n",
    "\n",
    "SFT Trainer Configuration:\n",
    "\n",
    "Uses text packing to maximize batch efficiency\n",
    "Enforces sequential data processing for reproducibility\n",
    "Handles the full train-evaluate-save workflow\n",
    "\n",
    "\n",
    "\n",
    "This configuration enables efficient fine-tuning while tracking model quality through perplexity measurements, allowing the training process to select the best-performing checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98e586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SFTTrainer...\n",
      "Unsloth: Hugging Face's packing is currently buggy - we're disabling it for now!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6080c41dae3b44fb9528e20ff10cd668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Hugging Face's packing is currently buggy - we're disabling it for now!\n",
      "SFTTrainer initialized.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    logits = torch.tensor(p.predictions)\n",
    "    labels = torch.tensor(p.label_ids)\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    perplexity = math.exp(loss.item())\n",
    "    # Clean up tensors used in calculation\n",
    "    del logits, labels, shift_logits, shift_labels, loss\n",
    "    return {\"perplexity\": perplexity}\n",
    "\n",
    "print(\"Defining training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optim=OPTIMIZER, \n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    seed=SEED,\n",
    "    save_strategy=EVALUATION_STRATEGY,\n",
    "    eval_strategy=EVALUATION_STRATEGY,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    load_best_model_at_end=True, \n",
    "    greater_is_better=False,\n",
    "    gradient_checkpointing=True, \n",
    "      save_total_limit=10, \n",
    ")\n",
    "\n",
    "print(\"Initializing SFTTrainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=1,\n",
    "    packing=True, \n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"SFTTrainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653ec20",
   "metadata": {},
   "source": [
    "Model Training Process\n",
    "This section executes the fine-tuning process and handles model saving, with appropriate error handling and memory management.\n",
    "Training Execution\n",
    "pythontrainer_stats = trainer.train()\n",
    "\n",
    "The code runs the training loop with several important safeguards:\n",
    "\n",
    "Memory Preparation:\n",
    "\n",
    "Forces garbage collection before training begins\n",
    "\n",
    "Clears CUDA cache to maximize available GPU memory\n",
    "\n",
    "Wraps training in try-except to handle potential failures\n",
    "\n",
    "\n",
    "Model Preservation:\n",
    "\n",
    "Saves the final model adapters and tokenizer configuration\n",
    "\n",
    "Creates a separate directory for the final model state\n",
    "\n",
    "Reports training statistics upon completion\n",
    "\n",
    "\n",
    "\n",
    "Best Model Tracking\n",
    "pythonif training_args.load_best_model_at_end:\n",
    "\n",
    "    best_model_checkpoint_path = trainer.state.best_model_checkpoint\n",
    "    \n",
    "The code implements a checkpoint management strategy:\n",
    "\n",
    "Records the path to the best-performing checkpoint based on evaluation metrics\n",
    "Issues a warning if the best model tracking is disabled\n",
    "Ensures the optimal model can be loaded for inference after training\n",
    "\n",
    "This training section represents the core computational work of the fine-tuning process, where the model adapters are adjusted to learn the tone-specific response patterns from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696f21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 10,000 | Num Epochs = 3 | Total steps = 3,750\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040/4,000,000,000 (1.05% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: ankitdev (ankitdev-chandigarh-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.4s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\papers-i-implement\\LoRA\\main_scripts\\wandb\\run-20250414_010935-e9jwnui2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ankitdev-chandigarh-university/huggingface/runs/e9jwnui2' target=\"_blank\">./results_optimized_all_tones_dynamic</a></strong> to <a href='https://wandb.ai/ankitdev-chandigarh-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ankitdev-chandigarh-university/huggingface' target=\"_blank\">https://wandb.ai/ankitdev-chandigarh-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ankitdev-chandigarh-university/huggingface/runs/e9jwnui2' target=\"_blank\">https://wandb.ai/ankitdev-chandigarh-university/huggingface/runs/e9jwnui2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 16:34:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.356879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.258892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.244021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.203526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.088400</td>\n",
       "      <td>1.189502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.088400</td>\n",
       "      <td>1.194674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.088400</td>\n",
       "      <td>1.162520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.088400</td>\n",
       "      <td>1.165029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.088400</td>\n",
       "      <td>1.147934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.965300</td>\n",
       "      <td>1.148425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.965300</td>\n",
       "      <td>1.131764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.965300</td>\n",
       "      <td>1.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.965300</td>\n",
       "      <td>1.182036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.965300</td>\n",
       "      <td>1.181832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.792800</td>\n",
       "      <td>1.177980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.792800</td>\n",
       "      <td>1.181986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.792800</td>\n",
       "      <td>1.198514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.792800</td>\n",
       "      <td>1.224114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.792800</td>\n",
       "      <td>1.214208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.663100</td>\n",
       "      <td>1.211881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.663100</td>\n",
       "      <td>1.220020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.663100</td>\n",
       "      <td>1.187294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.663100</td>\n",
       "      <td>1.183766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.663100</td>\n",
       "      <td>1.184301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.650100</td>\n",
       "      <td>1.177242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.650100</td>\n",
       "      <td>1.377867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.650100</td>\n",
       "      <td>1.465404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.650100</td>\n",
       "      <td>1.416843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.650100</td>\n",
       "      <td>1.446952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>1.463082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>1.451772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>1.456751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>1.486364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>1.459299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.359300</td>\n",
       "      <td>1.446282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.359300</td>\n",
       "      <td>1.462016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.359300</td>\n",
       "      <td>1.465536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "--- Training Complete ---\n",
      "Training Stats: TrainOutput(global_step=3750, training_loss=0.6746180623372395, metrics={'train_runtime': 59663.7118, 'train_samples_per_second': 0.503, 'train_steps_per_second': 0.063, 'total_flos': 2.4909770123083776e+17, 'train_loss': 0.6746180623372395})\n",
      "Saving final LoRA adapters to ./results_optimized_all_tones_dynamic/final_model\n",
      "Model adapters and tokenizer saved.\n",
      "Best model checkpoint found at: ./results_optimized_all_tones_dynamic\\checkpoint-1200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Train the Model ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "best_model_checkpoint_path = None \n",
    "try:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() \n",
    "    trainer_stats = trainer.train()\n",
    "    print(\"--- Training Complete ---\")\n",
    "    print(f\"Training Stats: {trainer_stats}\")\n",
    "    print(f\"Saving final LoRA adapters to {OUTPUT_DIR}/final_model\")\n",
    "    trainer.save_model(f\"{OUTPUT_DIR}/final_model\")\n",
    "    tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "    print(\"Model adapters and tokenizer saved.\")\n",
    "\n",
    "    if training_args.load_best_model_at_end:\n",
    "         best_model_checkpoint_path = trainer.state.best_model_checkpoint\n",
    "         print(f\"Best model checkpoint found at: {best_model_checkpoint_path}\")\n",
    "    else:\n",
    "         print(\"Warning: load_best_model_at_end=False. Post-training generation will use the *last* model state.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
